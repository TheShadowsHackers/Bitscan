#!/usr/bin/env python3
"""
Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…ÙØ§ØªÙŠØ­ Ø¨ÙŠØªÙƒÙˆÙŠÙ† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†ØªØ§Ø¦Ø¬
"""

import sys
import os
import time
import json
import argparse
import multiprocessing
import signal

import numpy as np
try:
    from tqdm import tqdm
except ImportError:
    tqdm = lambda x, **kwargs: x

from settings import *
from db_manager import DatabaseManager
from ai_utils import *

def print_logo():
    print(r"""
     ___    _    __  __ _      ____     ___   ____  _  __
    / _ \  / \  |  \/  | |    / ___|   / _ \ / ___|| |/ /
   | | | |/ _ \ | |\/| | |   | |  _   | | | | |    | ' / 
   | |_| / ___ \| |  | | |___| |_| |  | |_| | |___ | . \ 
    \___/_/   \_\_|  |_|____(_)____(_) \___/ \____||_|\_\
    """)

def signal_handler(signum, frame):
    print("\nğŸ›‘ [EXIT] Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙˆØ­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
    sys.exit(0)

signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

def update_molds_on_success(found_hex_key, target_addr, range_info, db_manager):
    """ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù†Ø¬Ø§Ø­ ÙˆØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£Ù†Ù…Ø§Ø· ÙˆØ§Ù„ØªØ¹Ù„Ù…."""
    pattern = extended_analyze_key(found_hex_key, start_hex=range_info.get("start"), stop_hex=range_info.get("stop"))
    pattern["found_key"] = found_hex_key
    pattern["target_addr"] = target_addr
    pattern["range"] = range_info
    pattern["found_time"] = int(time.time())
    db_manager.insert_pattern(pattern)
    db_manager.update_molds_stats(success=True)
    append_learning_log(pattern)

def update_molds_on_failure(target_addr, range_info, db_manager, tries=0):
    """ØªØ³Ø¬ÙŠÙ„ Ù…Ø­Ø§ÙˆÙ„Ø© ÙØ§Ø´Ù„Ø© Ø¶Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª."""
    history = {
        "target_addr": target_addr,
        "range": range_info,
        "success": False,
        "tries": tries,
        "time_spent": 0,
        "mode": "AI",
        "timestamp": int(time.time())
    }
    db_manager.insert_history(history)
    db_manager.update_molds_stats(success=False)

def log_training(success, priv_hex, target_addr, range_info, tries, time_spent, mode, db_manager):
    """ØªØ³Ø¬ÙŠÙ„ Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ¯Ø±ÙŠØ¨ ÙÙŠ Ø¬Ø¯ÙˆÙ„ Ù…Ø®ØµØµ."""
    entry = {
        "success": success,
        "found_key": priv_hex,
        "target_addr": target_addr,
        "range": range_info,
        "tries": tries,
        "mode": mode,
        "time_spent": time_spent,
        "timestamp": int(time.time())
    }
    db_manager.insert_training(entry)

def get_training_stats(db_manager, current_range, mode):
    """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø³Ø±ÙŠØ¹Ø© Ø­ÙˆÙ„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨."""
    records = db_manager.get_training_records(mode)
    if not records:
        return 0.0, 0.0, None
    total = len(records)
    successes = [x for x in records if x["success"]]
    success_rate = (len(successes) / total) * 100 if total else 0.0
    avg_time = float(np.mean([x["time_spent"] for x in successes])) if successes else 0.0
    last_similar = None
    best_diff = float('inf')
    cur_start = int(current_range["start"], 16)
    cur_stop = int(current_range["stop"], 16)
    cur_size = cur_stop - cur_start
    for x in records:
        try:
            size = int(x["range_stop"], 16) - int(x["range_start"], 16)
            diff = abs(size - cur_size)
            if diff < best_diff:
                best_diff = diff
                last_similar = x
        except:
            continue
    return success_rate, avg_time, last_similar

def get_all_patterns(db_manager):
    return db_manager.get_recent_patterns(30)

def show_learning_level(db_manager):
    """Ø·Ø¨Ø§Ø¹Ø© Ù…Ù„Ø®Øµ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ¹Ù„Ù…"""
    stats = db_manager.get_stats()
    patterns = db_manager.get_recent_patterns(30)
    print("\n==============================")
    print("ğŸ“ˆ AI Knowledge Progress")
    print(f"ğŸ”‘ Successful keys: {stats.get('total_success', 0)}")
    print(f"âŒ Failed attempts: {stats.get('total_fail', 0)}")
    print(f"ğŸ§  Learned patterns: {len(patterns)}")
    if patterns:
        last = patterns[0]
        print("ğŸ“ Last learned key pattern:")
        for k, v in last.items():
            if k not in ("found_key", "id"):
                print(f"   {k}: {v}")
    else:
        print("No patterns learned yet.")
    print("==============================\n")

def ai_confidence_indicator(this_range, db_manager):
    """Ù…Ø¤Ø´Ø± Ø«Ù‚Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙÙŠ Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ù…ÙØªØ§Ø­."""
    all_patterns = get_all_patterns(db_manager)
    if not all_patterns:
        return 0
    cur_start = int(this_range["start"], 16)
    cur_stop = int(this_range["stop"], 16)
    cur_size = cur_stop - cur_start
    similar = 0
    for p in all_patterns:
        try:
            s = int(p.get("range_start", "0"), 16)
            e = int(p.get("range_stop", "0"), 16)
            size = e - s
            diff = abs(size - cur_size)
            if diff < (cur_size // 4):
                similar += 1
        except:
            continue
    sr, _, _ = get_training_stats(db_manager, this_range, "AI")
    conf = min(100, int((similar * 20) + (sr * 0.7)))
    return conf

def make_run_report(report, db_manager):
    """ØªØ®Ø²ÙŠÙ† ØªÙ‚Ø±ÙŠØ± Ø¬Ù„Ø³Ø© Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª."""
    report_json = json.dumps(report, indent=2, ensure_ascii=False)
    db_manager.insert_report_log(report_json)
    print("\nğŸ“‹ ØªÙ… Ø­ÙØ¸ ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø¬Ù„Ø³Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¶Ù…Ù† report_logs.\n")

def search_worker(start, stop, queue, process_id, mode_name, db_path, target_address):
    """ÙˆØ¸ÙŠÙØ© ÙƒÙ„ Ø¹Ù…Ù„ÙŠØ© ÙØ±Ø¹ÙŠØ© Ù„Ù„Ø¨Ø­Ø«."""
    db_manager = DatabaseManager(db_path)
    tried = 0
    found = False
    checked = set()
    local_report = {
        "process_id": process_id,
        "mode": mode_name,
        "tries": 0,
        "found": False,
        "found_key": None,
        "found_address": None,
        "errors": [],
        "start": start,
        "stop": stop
    }
    last_update = 0
    while not found:
        try:
            all_patterns = get_all_patterns(db_manager)
            if mode_name == "AI":
                batch_size = 50000
                candidates = ai_candidate_generator(start, stop, batch_size, all_patterns)
            elif mode_name == "Genetic":
                candidates = genetic_candidates(start, stop, population=50000)
            elif mode_name == "GPU":
                batch_size = 2000000
                try:
                    import cupy as cp
                    cp.cuda.Device(0).use()
                    candidates = cp.random.randint(start, stop + 1, size=int(batch_size * 2), dtype=cp.int64)
                    scores = cp.random.uniform(0, 1, size=candidates.shape)
                    indices = cp.argsort(scores)[::-1]
                    candidates = cp.asnumpy(candidates[indices])[:batch_size].tolist()
                except Exception as e:
                    candidates = ai_candidate_generator(start, stop, batch_size, all_patterns)
            else:
                candidates = list(range(start, stop + 1))
            for priv_int in candidates:
                if priv_int in checked:
                    continue
                checked.add(priv_int)
                priv_key_hex = hex(priv_int)[2:].zfill(64)
                priv_bytes = bytes.fromhex(priv_key_hex)
                try:
                    address = privatekey_to_address(priv_bytes)
                except Exception as e:
                    continue
                if address == target_address:
                    queue.put((True, priv_key_hex, address, process_id, tried, mode_name))
                    local_report.update({
                        "found": True,
                        "found_key": priv_key_hex,
                        "found_address": address
                    })
                    found = True
                    break
                tried += 1
                if tried - last_update >= 5000:
                    queue.put((False, None, None, process_id, tried - last_update, mode_name))
                    last_update = tried
        except Exception as e:
            local_report["errors"].append(str(e))
            continue
    local_report["tries"] = tried
    tmp_report = f"proc_report_{process_id}.json"
    with open(tmp_report, "w", encoding="utf-8") as f:
        json.dump(local_report, f, indent=2, ensure_ascii=False)
    if not found:
        queue.put((False, None, None, process_id, tried, mode_name))
    db_manager.close()

def manager(start, stop, num_processes, modes, db_path, target_address):
    """Ø¥Ø¯Ø§Ø±Ø© ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª ÙˆØ§Ø³ØªÙ„Ø§Ù… Ø§Ù„Ù†ØªØ§Ø¦Ø¬."""
    manager_mp = multiprocessing.Manager()
    queue = manager_mp.Queue()
    total_tries = 0
    found = False
    step = (stop - start + 1) // num_processes
    processes = []
    session_start_time = int(time.time())
    for i in range(num_processes):
        s = start + i * step
        e = start + (i + 1) * step - 1 if i < num_processes - 1 else stop
        mode = modes[i % len(modes)]
        p = multiprocessing.Process(target=search_worker, args=(s, e, queue, i + 1, mode, db_path, target_address))
        processes.append(p)
        p.start()

    pbar = tqdm(total=stop - start + 1, desc="ğŸ” Adaptive Search", ncols=80)
    t0 = time.time()
    final_report = {
        "success": False,
        "target_address": target_address,
        "range": {"start": hex(start), "stop": hex(stop)},
        "process_reports": [],
        "winner_process": None,
        "winner_key": None,
        "winner_address": None,
        "winner_mode": None,
        "winner_tries": None,
        "time_spent": None,
        "errors": []
    }
    found_mode = None

    while not found and any(p.is_alive() for p in processes):
        try:
            result = queue.get(timeout=1)
            is_found, priv, addr, proc_id, delta, mode = result
            proc_report_file = f"proc_report_{proc_id}.json"
            proc_report = {}
            if os.path.exists(proc_report_file):
                with open(proc_report_file, encoding="utf-8") as f:
                    proc_report = json.load(f)
                final_report["process_reports"].append(proc_report)
                os.remove(proc_report_file)
            else:
                proc_report = {"process_id": proc_id, "mode": mode, "tries": delta, "found": is_found}
                final_report["process_reports"].append(proc_report)
            if not is_found:
                total_tries += delta
                pbar.update(delta)
            else:
                t1 = time.time()
                elapsed = t1 - t0
                print(f"\n\nğŸ¯ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙØªØ§Ø­ ÙÙŠ Ø§Ù„Ø¹Ù…Ù„ÙŠØ© #{proc_id} [{mode} Mode]")
                print("Private Key (HEX):", priv[:8] + "..." + priv[-8:])  # Ø¥Ø¸Ù‡Ø§Ø± Ø¬Ø²Ø¡ ÙÙ‚Ø· Ù„Ù„Ø­Ù…Ø§ÙŠØ©
                print("Address:", addr)
                with open(FOUND_KEY_FILE, "w", encoding="utf-8") as f:
                    f.write(f"Private Key (HEX): {priv}\n")
                    f.write(f"Address: {addr}\n")
                update_molds_on_success(priv, target_address, {"start": hex(start), "stop": hex(stop)}, DatabaseManager(db_path))
                log_training(True, priv, target_address, {"start": hex(start), "stop": hex(stop)}, total_tries, elapsed, mode, DatabaseManager(db_path))
                found = True
                found_mode = mode
                final_report.update({
                    "success": True,
                    "winner_process": proc_id,
                    "winner_key": priv,
                    "winner_address": addr,
                    "winner_mode": mode,
                    "winner_tries": total_tries,
                    "time_spent": elapsed
                })
                break
        except Exception as e:
            final_report["errors"].append(str(e))
            continue

    for p in processes:
        p.terminate()
    for p in processes:
        p.join()
    pbar.close()
    if not found:
        elapsed = time.time() - t0
        print(f"\nğŸš« Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù‡Ø¯Ù Ø¶Ù…Ù† Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ù…Ø­Ø¯Ø¯ (ØªÙ… ØªØ¬Ø±Ø¨Ø© ~{total_tries:,} Ù…ÙØªØ§Ø­ØŒ Ø§Ù„ÙˆÙ‚Øª: {elapsed:.2f} Ø«Ø§Ù†ÙŠØ©).")
        update_molds_on_failure(target_address, {"start": hex(start), "stop": hex(stop)}, DatabaseManager(db_path), total_tries)
        log_training(False, None, target_address, {"start": hex(start), "stop": hex(stop)}, total_tries, elapsed, "AI", DatabaseManager(db_path))
        final_report["time_spent"] = elapsed

    session_end_time = int(time.time())
    dbm = DatabaseManager(db_path)
    dbm.insert_search_session({
        "session_start": session_start_time,
        "session_end": session_end_time,
        "range": {"start": hex(start), "stop": hex(stop)},
        "mode": found_mode if found_mode else "None",
        "total_tries": total_tries,
        "found": found,
        "winner_key": final_report.get("winner_key"),
        "winner_address": final_report.get("winner_address")
    })
    dbm.close()
    make_run_report(final_report, DatabaseManager(db_path))

def main():
    print_logo()
    parser = argparse.ArgumentParser(description="AI Key Search and Learning Tool")
    parser.add_argument('--mode', type=str, default='scan', choices=['scan', 'train', 'export-db', 'import-db'],
                        help="ÙˆØ¶Ø¹ Ø§Ù„ØªØ´ØºÙŠÙ„")
    parser.add_argument('--db', type=str, default=DEFAULT_DB_PATH, help="Ù…Ø³Ø§Ø± Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª SQLite")
    parser.add_argument('--target', type=str, default=DEFAULT_TARGET_ADDRESS, help="Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù‡Ø¯Ù (Ø¨ÙŠØªÙƒÙˆÙŠÙ†)")
    parser.add_argument('--start', type=str, default=DEFAULT_START_HEX, help="Ù…ÙØªØ§Ø­ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¨Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø³Ø§Ø¯Ø³ Ø¹Ø´Ø±")
    parser.add_argument('--stop', type=str, default=DEFAULT_STOP_HEX, help="Ù…ÙØªØ§Ø­ Ø§Ù„Ù†Ù‡Ø§ÙŠØ© Ø¨Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø³Ø§Ø¯Ø³ Ø¹Ø´Ø±")
    parser.add_argument('--export-file', type=str, default=EXPORT_DB_FILE, help="Ù…Ù„Ù Ø§Ù„ØªØµØ¯ÙŠØ±")
    parser.add_argument('--import-file', type=str, default=IMPORT_DB_FILE, help="Ù…Ù„Ù Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯")
    args = parser.parse_args()

    db_path = args.db
    db_manager = DatabaseManager(db_path)
    target_address = args.target
    start_hex = args.start
    stop_hex = args.stop

    save_key_range_info(target_address, start_hex, stop_hex)

    if args.mode == 'export-db':
        print("ØªØµØ¯ÙŠØ± Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ù…Ù„Ù Ø®Ø§Ø±Ø¬ÙŠ...")
        db_manager.export_db(args.export_file)
        print(f"ØªÙ… ØªØµØ¯ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ {args.export_file}")
    elif args.mode == 'import-db':
        print("Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ù…Ù„Ù Ø®Ø§Ø±Ø¬ÙŠ...")
        db_manager.import_db(args.import_file)
        print(f"ØªÙ… Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† {args.import_file}")
    elif args.mode == 'train':
        print("ØªØ´ØºÙŠÙ„ ÙˆØ¶Ø¹ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (ØºÙŠØ± Ù…ÙØ¹Ù„ Ø¹Ù…Ù„ÙŠÙ‹Ø§ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø¥ØµØ¯Ø§Ø±) ...")
    else:  # ÙˆØ¶Ø¹ Ø§Ù„Ø¨Ø­Ø« (scan)
        show_learning_level(db_manager)
        start_int = int(start_hex, 16)
        stop_int = int(stop_hex, 16)
        num_processes = os.cpu_count() or 4
        # ÙƒØ´Ù ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ø¯Ø¹Ù… GPU
        try:
            import cupy as cp
            modes = ["GPU"] + ["AI"] * (num_processes - 1)
        except ImportError:
            modes = ["AI"] * num_processes
        print(f"\nTarget address: {target_address}")
        print(f"Range: {hex(start_int)} -> {hex(stop_int)}")
        print(f"Processes: {num_processes}")
        print("Modes:", modes)
        ai_conf = ai_confidence_indicator({"start": hex(start_int), "stop": hex(stop_int)}, db_manager)
        print(f"\nğŸ§  AI Solve Confidence: {ai_conf}%\n")
        sr, at, ls = get_training_stats(db_manager, {"start": hex(start_int), "stop": hex(stop_int)}, "AI")
        print(f"ğŸ“Š [AI Mode] Ù†Ø³Ø¨Ø© Ø§Ù„Ù†Ø¬Ø§Ø­ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©: {sr:.2f}% | Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹: {at:.1f} Ø«Ø§Ù†ÙŠØ©")
        print("Searching... (Ctrl+C to stop)\n")
        manager(start_int, stop_int, num_processes, modes, db_path, target_address)
        export_all_patterns_to_file(db_manager)
    db_manager.close()

if __name__ == "__main__":
    multiprocessing.freeze_support()
    main()